{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FPoBe07T8Pox"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.stats as stats\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_curve\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "Mo8jQ1nh628B",
        "outputId": "241db1b2-0701-4aa6-cd8e-774cc8a9b594"
      },
      "outputs": [],
      "source": [
        "# Load the CSV file into a DataFrame\n",
        "training_dataset = pd.read_csv('dataset/customer_churn_dataset-training-master.csv')\n",
        "testing_dataset = pd.read_csv('dataset/customer_churn_dataset-testing-master.csv')\n",
        "# Display the first 5 rows of the DataFrame\n",
        "# print(df1.head())\n",
        "# print(df1.columns)\n",
        "df = pd.concat([training_dataset, testing_dataset], ignore_index=True)\n",
        "\n",
        "df.head(50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "id": "huQOnasR7Y9n",
        "outputId": "f3273235-dbee-43df-c209-e96bfac837d3"
      },
      "outputs": [],
      "source": [
        "#Inspect the churn column\n",
        "df['Churn']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zaUcSKleBeoY"
      },
      "source": [
        "After looking through the dataset, row 199295 has missing values for all the columns. Therefore, we can remove this row."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "y0Iyv554DQ51",
        "outputId": "34180d1c-d1c9-4d9a-cb66-010271965eec"
      },
      "outputs": [],
      "source": [
        "#Recheck for null values\n",
        "nan_count = np.sum(df.isnull(), axis=0)\n",
        "nan_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.drop(columns=[\"CustomerID\"],inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# sns.lineplot(x=df['Age'], y=df['Churn'])\n",
        "# sns.lineplot(x=training_dataset['Age'], y=training_dataset['Churn'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "id": "9fn-MWNpDYr6",
        "outputId": "a0b2de1a-9e03-4a0b-95db-23284c78dea9"
      },
      "outputs": [],
      "source": [
        "#Identifying correlations with the label\n",
        "corr_matrix = round(df.corr(),5)\n",
        "corrs = corr_matrix['Churn']\n",
        "corrs_sorted = corrs.sort_values(ascending=False)\n",
        "corrs_sorted\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 775
        },
        "id": "ZFXvjmM8FTlY",
        "outputId": "62774232-06b0-491c-fa76-949989c7171f"
      },
      "outputs": [],
      "source": [
        "# #Visualize the top two correlated features\n",
        "# df_sample = df.sample(n=30000)\n",
        "# top_two_corr = list(corrs_sorted[2:4].index)\n",
        "# df_corrs_sample = df_sample[top_two_corr].copy()\n",
        "# df_corrs_sample['Churn'] = df_sample['Churn']\n",
        "# sns.pairplot(data=df_corrs_sample, kind='kde', corner=True)\n",
        "# #ASK TA about this"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# filter_df = df[df['Age']>50]\n",
        "\n",
        "# filter_df\n",
        "# filter_df.hist(column='Churn')\n",
        "\n",
        "# training_dataset[training_dataset['Age']>50].hist(column='Churn')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Normalize the numerical features by scaling values between 0 and 1\n",
        "# scaler = MinMaxScaler()\n",
        "# df = scaler.fit_transform(df)\n",
        "# df_norm_test = pd.DataFrame(df_norm_test, columns=df_testing.columns)\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Spltting the concatenated data into 80/20 training and testing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Spltting the concatenated data into 80/20 training and testing\n",
        "from sklearn.model_selection import train_test_split\n",
        "y = df['Churn']\n",
        "X = df.drop(columns='Churn')\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=1234)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "predictions = model.predict(X_test)\n",
        "probabilities = model.predict_proba(X_test)\n",
        "lg_loss = log_loss(y_test, probabilities)\n",
        "acc_score = accuracy_score(y_test, predictions)\n",
        "precision = precision_score(y_test, predictions)\n",
        "recall = recall_score(y_test, predictions)\n",
        "# print('Log loss=' + str(lg_loss) + ', accuracy score: ' + str(acc_score), ', recall : ' + recall)\n",
        "print(f'Log Loss: {lg_loss}, accuracy score: {acc_score}, precision: {precision}, recall: {recall}')\n",
        "print('F1 Score: ' + str((2*precision*recall)/(precision+recall)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#get the coefficients from the logistic regression model\n",
        "coefficients = model.coef_[0]\n",
        "feature_names = X_train.columns\n",
        "\n",
        "coef_df = pd.DataFrame({'Feature': feature_names, 'Coefficient': coefficients})\n",
        "\n",
        "print(coef_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Histograms of churn and no churn in the newly split training dataset\n",
        "y_train_named = y_train.rename('Churn')\n",
        "training_data = pd.concat([X_train, y_train_named], axis=1)\n",
        "\n",
        "plt.figure(figsize=(6, 3))\n",
        "\n",
        "features = training_data.drop(columns=['Churn']).columns\n",
        "\n",
        "for feature in features:\n",
        "    plt.figure(figsize=(6, 3))\n",
        "    \n",
        "    sns.histplot(training_data[training_data['Churn'] == 0][feature], color='blue', label='No Churn', kde=False, stat=\"density\", bins=30, alpha=0.5)\n",
        "    sns.histplot(training_data[training_data['Churn'] == 1][feature], color='red', label='Churn', kde=False, stat=\"density\", bins=30, alpha=0.5)\n",
        "    \n",
        "    plt.title(f'Distribution of {feature} for Churn vs No Churn')\n",
        "    plt.xlabel(feature)\n",
        "    plt.ylabel('Density')\n",
        "    plt.legend()\n",
        "    \n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Stratifying the 80/20 training and testing split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "y = df['Churn']\n",
        "X = df.drop(columns='Churn')\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,stratify=y,random_state=1234)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#checking imbalance: \n",
        "y_train.value_counts(normalize=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "predictions = model.predict(X_test)\n",
        "probabilities = model.predict_proba(X_test)\n",
        "lg_loss = log_loss(y_test, probabilities)\n",
        "acc_score = accuracy_score(y_test, predictions)\n",
        "precision = precision_score(y_test, predictions)\n",
        "recall = recall_score(y_test, predictions)\n",
        "# print('Log loss=' + str(lg_loss) + ', accuracy score: ' + str(acc_score), ', recall : ' + recall)\n",
        "print(f'Log Loss: {lg_loss}, accuracy score: {acc_score}, precision: {precision}, recall: {recall}')\n",
        "print('F1 Score: ' + str((2*precision*recall)/(precision+recall)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#get the coefficients from the logistic regression model\n",
        "coefficients = model.coef_[0]\n",
        "feature_names = X_train.columns\n",
        "\n",
        "coef_df = pd.DataFrame({'Feature': feature_names, 'Coefficient': coefficients})\n",
        "\n",
        "print(coef_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Histograms of churn and no churn in the newly split training dataset\n",
        "y_train_named = y_train.rename('Churn')\n",
        "training_data = pd.concat([X_train, y_train_named], axis=1)\n",
        "\n",
        "plt.figure(figsize=(6, 3))\n",
        "\n",
        "features = training_data.drop(columns=['Churn']).columns\n",
        "\n",
        "for feature in features:\n",
        "    plt.figure(figsize=(6, 3))\n",
        "    \n",
        "    sns.histplot(training_data[training_data['Churn'] == 0][feature], color='blue', label='No Churn', kde=False, stat=\"density\", bins=30, alpha=0.5)\n",
        "    sns.histplot(training_data[training_data['Churn'] == 1][feature], color='red', label='Churn', kde=False, stat=\"density\", bins=30, alpha=0.5)\n",
        "    \n",
        "    plt.title(f'Distribution of {feature} for Churn vs No Churn')\n",
        "    plt.xlabel(feature)\n",
        "    plt.ylabel('Density')\n",
        "    plt.legend()\n",
        "    \n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Histograms of churn and no churn in the newly split testing dataset\n",
        "y_test_named = y_test.rename('Churn')\n",
        "testing_data = pd.concat([X_test, y_test_named], axis=1)\n",
        "\n",
        "plt.figure(figsize=(6, 3))\n",
        "\n",
        "features = testing_data.drop(columns=['Churn']).columns\n",
        "\n",
        "for feature in features:\n",
        "    plt.figure(figsize=(6, 3))\n",
        "    \n",
        "    sns.histplot(testing_data[testing_data['Churn'] == 0][feature], color='blue', label='No Churn', kde=False, stat=\"density\", bins=30, alpha=0.5)\n",
        "    sns.histplot(testing_data[testing_data['Churn'] == 1][feature], color='red', label='Churn', kde=False, stat=\"density\", bins=30, alpha=0.5)\n",
        "    \n",
        "    plt.title(f'Distribution of {feature} for Churn vs No Churn')\n",
        "    plt.xlabel(feature)\n",
        "    plt.ylabel('Density')\n",
        "    plt.legend()\n",
        "    \n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creating a decision tree model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_stats(y_test, y_pred, probabilities):\n",
        "    acc_score = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    lg_loss = log_loss(y_test, probabilities)\n",
        "    print(f'Log Loss: {lg_loss}, accuracy score: {acc_score}, precision: {precision}, recall: {recall}')\n",
        "    print('F1 Score: ' + str((2*precision*recall)/(precision+recall)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "y = df['Churn']\n",
        "X = df.drop(columns='Churn')\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4,stratify=y,random_state=1234)\n",
        "\n",
        "dt = DecisionTreeClassifier(criterion='log_loss', max_depth=7, min_samples_leaf=4, random_state=1234)\n",
        "dt.fit(X_train, y_train)\n",
        "\n",
        "y_pred = dt.predict(X_test)\n",
        "probabilities = dt.predict_proba(X_test)\n",
        "\n",
        "get_stats(y_test, y_pred, probabilities)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'max_depth': [3, 5, 7],  # Testing different tree depths\n",
        "    'min_samples_leaf': [2, 4]  # Minimum samples required at leaf nodes\n",
        "}\n",
        "\n",
        "# Perform Grid Search with 5-fold cross-validation\n",
        "grid_search = GridSearchCV(estimator=dt, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Train the model using grid search\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best hyperparameters and model\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "print(f'{best_params}, {best_model}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "feature_names = X.columns\n",
        "feature_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "from sklearn import datasets\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn import tree\n",
        "\n",
        "text_representation = tree.export_text(dt, feature_names=feature_names)\n",
        "print(text_representation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(200,50))\n",
        "_ = tree.plot_tree(dt,\n",
        "                   feature_names=feature_names,\n",
        "                   class_names=['No Churn','Churn'],\n",
        "                   filled=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Trying a Random forest model now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# RandomForestClassifier(criterion='entropy', n_estimators=20)\n",
        "\n",
        "\n",
        "y = df['Churn']\n",
        "X = df.drop(columns='Churn')\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4,stratify=y,random_state=1234)\n",
        "\n",
        "rf = RandomForestClassifier(criterion='log_loss', n_estimators=20)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = rf.predict(X_test)\n",
        "probabilities = rf.predict_proba(X_test)\n",
        "\n",
        "get_stats(y_test, y_pred, probabilities)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hist Gradient Boosting CLassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "# from sklearn.datasets import make_hastie_10_2\n",
        "\n",
        "y = df['Churn']\n",
        "X = df.drop(columns='Churn')\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4,stratify=y,random_state=1234)\n",
        "\n",
        "hgbc = HistGradientBoostingClassifier(loss='log_loss', max_iter=100)\n",
        "hgbc.fit(X_train, y_train)\n",
        "\n",
        "y_pred = hgbc.predict(X_test)\n",
        "probabilities = hgbc.predict_proba(X_test)\n",
        "\n",
        "get_stats(y_test, y_pred, probabilities)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "y = df['Churn']\n",
        "X = df.drop(columns='Churn')\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4,stratify=y,random_state=1234)\n",
        "\n",
        "xgb_clf = xgb.XGBClassifier(\n",
        "    objective='binary:hinge',\n",
        "    eval_metric='auc',\n",
        "    # 'objective': 'binary:logistic',\n",
        "    # 'eval_metric': 'auc',\n",
        "    # 'eta': 0.1,\n",
        "    # 'max_depth': 6,\n",
        "    # 'min_child_weight': 1,\n",
        "    # 'subsample': 0.8,\n",
        "    # 'colsample_bytree': 0.8,\n",
        "    # 'lambda': 1.0,\n",
        "    # 'alpha': 0.0,\n",
        "    # 'n_estimators': 500\n",
        ")\n",
        "\n",
        "xgb_clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = xgb_clf.predict(X_test)\n",
        "\n",
        "get_stats(y_test, y_pred)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
